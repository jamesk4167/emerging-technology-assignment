{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook I plan to create a Neural Network which can recognize hand writted digits. I will be using the Mnist DataSet to train and test the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly in order to train our data we have to import it from our data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**warning: In order to run these files you must first download the training sets, see the reference in mnist data sets for downloads used in this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "    \n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as kr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "\n",
    "model.add = (kr.layers.Dense(units = 512, activation = 'relu', input_dim = 784))\n",
    "model.add = (kr.layers.Dense(units = 512, activation = 'relu'))\n",
    "model.add = (kr.layers.Dense(units = 10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "    \n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()\n",
    "    \n",
    "    \n",
    "training_images = ~np.array(list(train_img[16:])).reshape(60000, 28, 28).astype(np.uint8) / 255.0\n",
    "training_labels =  np.array(list(train_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = training_images.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as pre\n",
    "\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(training_labels)\n",
    "outputs = encoder.transform(training_labels)\n",
    "\n",
    "print(training_labels[0], outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 354s 6ms/step - loss: 0.4343 - acc: 0.8595\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 279s 5ms/step - loss: 0.2441 - acc: 0.9239\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 242s 4ms/step - loss: 0.1995 - acc: 0.9377\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 272s 5ms/step - loss: 0.1733 - acc: 0.9457\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 253s 4ms/step - loss: 0.1557 - acc: 0.9513\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 265s 4ms/step - loss: 0.1482 - acc: 0.9536\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 264s 4ms/step - loss: 0.1401 - acc: 0.9574\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 259s 4ms/step - loss: 0.1315 - acc: 0.9595\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 427s 7ms/step - loss: 0.1247 - acc: 0.9619 1s -\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 452s 8ms/step - loss: 0.1232 - acc: 0.9620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17cab8959e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(inputs, outputs, epochs = 10, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I am training the neural network. I decided to take 10 epochs which basically means iterating through the dataset 10 times and took a batch size of 10 so my images would be fed into the neural network 10 at a time. Here we can atleast see it is working on the training set as the accuracy is getting higher everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_img = f.read()\n",
    "\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_lbl = f.read()\n",
    "    \n",
    "test_images = ~np.array(list(test_img[16:])).reshape(10000, 784).astype(np.uint8) / 255.0\n",
    "test_labels =  np.array(list(test_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9469"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(encoder.inverse_transform(model.predict(test_images)) == test_labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
